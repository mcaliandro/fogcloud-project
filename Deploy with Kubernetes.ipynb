{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy with Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed explanations, see [README](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Kubernetes multi-node cluster is required to deploy the application.\n",
    "\n",
    "Check if a cluster, called \"smarthome\", already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export CLUSTER=smarthome\n",
    "\n",
    "# list the clusters\n",
    "kind get clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it doesn't exist, then create it with Kind by using the manifest ```paas/00-kind_create_cluster.yml```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind create cluster --config paas/00-kind_create_cluster.yml --name $CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the K8s connection to the cluster \"smarthome\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "export KUBECONFIG=\"$(kind get kubeconfig-path --name=$CLUSTER)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl cluster-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application is delivered via Docker images that are stored into a private repository on DockerHub. So, perform the login to DockerHub via Docker login utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### already logged in\n",
    "#docker login --username=$USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a Kubernetes Secret object to allow the pulling of images from the private repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create secret generic regcred \\\n",
    "    --from-file=.dockerconfigjson=$HOME/.docker/config.json \\\n",
    "    --type=kubernetes.io/dockerconfigjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Persistent Volume\n",
    "\n",
    "Use the manifest ```paas/01-k8s_create_volume.yml``` that describes 3 Kubernetes resources:\n",
    "- **StorageClass** defines policies for *binding* and *provisioning* the volume\n",
    "- **PersistentVolume** defines the *node affinity* and the *access mode* to the volume\n",
    "- **PersistentVolumeClaim** used to specify that a node uses the volume\n",
    "\n",
    "Manually choose a worker node that hasn't been scheduled yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKER=\"${CLUSTER}-worker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mount point inside the chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it $WORKER sh -c \"mkdir /mnt/volume\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the folder has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it $WORKER sh -c \"ls /mnt\"\n",
    "unset WORKER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the manifest ```paas/01-k8s_create_volume.yml``` to specify the mount point and the correct node reference:\n",
    "\n",
    "```bash \n",
    "vi paas/01-k8s_create_volume.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the persistent volume and its claim with k8s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f paas/01-k8s_create_volume.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the storage classes\n",
    "kubectl get sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the persistent volumes\n",
    "kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the claims\n",
    "kubectl get pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Datastore\n",
    "\n",
    "Use the manifest ```paas/02-k8s_deploy_datastore.yml``` that describes 2 Kubernetes resources:\n",
    "\n",
    "- **StatefulSet** defines one standalone stateful replica (1 Pod)\n",
    "- **Service** defines a headless service (required by StatefulSet)\n",
    "\n",
    "Deploy Datastore with K8s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f paas/02-k8s_deploy_datastore.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the services\n",
    "kubectl get svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the stateful sets\n",
    "kubectl get statefulset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the pods with label app=datastore\n",
    "kubectl get pods -l app=datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the persistent volume has been correctly binded and provisioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the node's name\n",
    "NODE_NAME=\"$(kubectl get pod -l app=datastore -o json | jq '.items[0].spec.nodeName' | tr -d '\"')\"\n",
    "\n",
    "# get the Pod's hostname\n",
    "POD_NAME=\"$(kubectl get pod -l app=datastore -o json | jq '.items[0].spec.hostname' | tr -d '\"')\"\n",
    "\n",
    "# inspect the mount point on the node container\n",
    "docker exec -it $NODE_NAME sh -c \"ls /mnt/volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the mounted volume on the Pod\n",
    "kubectl exec -it $POD_NAME -- sh -c \"ls /data/db\"\n",
    "unset POD_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a taint \"NoSchedule\" on the node where Datastore Pod is scheduled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl taint node $NODE_NAME node-role.kubernetes.io/master=value:NoSchedule\n",
    "unset NODE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Populate the Datastore\n",
    "\n",
    "Use the manifest ```paas/03-k8s_create_pod_initdb.yml``` to create a Kubernetes Pod in charge of populating the db directly inside the cluster.\n",
    "\n",
    "Create the Pod from a Docker image that includes a Python script, for querying the db, and a basic set of information to move inside the db.\n",
    "\n",
    "Retrieve the connection parameters to Datastore (Pod's hostname and MongoDB's port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IP address of the Datastore Pod\n",
    "export DB_HOST=\"$(kubectl get pod -l app=datastore -o json | jq '.items[0].status.podIP' | tr -d '\"')\"\n",
    "\n",
    "# get the port exposed by the Mongo container\n",
    "export DB_PORT=\"$(kubectl get pod -l app=datastore -o json | jq '.items[0].spec.containers[0].ports[0].containerPort' | tr -d '\"')\"\n",
    "\n",
    "echo \"Pod hostname\" $DB_HOST\n",
    "echo \"Exposed port\" $DB_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the manifest ```paas/03-k8s_create_pod_initdb.yml``` so that it includes the references to the Datastore Pod:\n",
    "\n",
    "```bash\n",
    "vi paas/03-k8s_create_pod_initdb.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Pod with K8s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f paas/03-k8s_create_pod_initdb.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it the Pod is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pod initdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually execute the Python script within the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it initdb -- sh -c \"python init.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished, delete the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete pod initdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the running Pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy the Corestack\n",
    "\n",
    "Use the manifest ```paas/04-k8s_deploy_core.yml``` that describes 2 Kubernetes resources:\n",
    "\n",
    "- **Deployment** setups the ReplicaSet and the service discovery mode (via environment variables)\n",
    "- **Service** uses NodePort to publish the Core service by allocating a static port\n",
    "\n",
    "Retrieve the connection parameters to Datastore (pod-hostname:mongo-port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Pod hostname\" $DB_HOST\n",
    "echo \"Exposed port\" $DB_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the manifest ```paas/04-k8s_deploy_core.yml``` to specify the environment variables ```DB_HOST``` and ```DB_PORT```:\n",
    "\n",
    "```bash\n",
    "vi paas/04-k8s_deploy_core.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the Core stack with K8s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f paas/04-k8s_deploy_core.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the service with label app=core\n",
    "kubectl get svc -l app=core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the deployment\n",
    "kubectl get deploy -l app=core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the ReplicaSet\n",
    "kubectl get rs -l app=core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show where the Pods have been scheduled\n",
    "kubectl get pods -l app=core -o json | jq '.items[] | {name: .metadata.name, node: .spec.nodeName}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the connection to Core service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IP address of the master node\n",
    "export MASTER_NODE=\"$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' \"$CLUSTER-control-plane\")\"\n",
    "\n",
    "# get the port exposed by Core service\n",
    "export CORE_PORT=\"$(kubectl get svc -l app=core -o json | jq '.items[0].spec.ports[0].nodePort')\"\n",
    "\n",
    "# perform a \"GET /\" on Core APIs\n",
    "curl http://$MASTER_NODE:$CORE_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploy the Dashboard\n",
    "\n",
    "Use the manifest ```paas/05-k8s_deploy_dashboard.yml``` that describes 2 Kubernetes resources:\n",
    "\n",
    "- **Deployment** setups the ReplicaSet and the service discovery mode (via environment variables)\n",
    "- **Service** uses NodePort to publish the Dashboard service by allocating a static port\n",
    "\n",
    "Retrieve the connection parameters to Corestack (master node IP address and Core service port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Master node IP\" $MASTER_NODE\n",
    "echo \"Corestack port\" $CORE_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the manifest ```paas/05-k8s_deploy_dashboard.yml``` to specify the environment variables ```CORESTACK_HOST``` and ```CORESTACK_PORT```:\n",
    "\n",
    "```bash\n",
    "vi paas/05-k8s_deploy_dashboard.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the Dashboard with K8s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f paas/05-k8s_deploy_dashboard.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the service with label app=dashboard\n",
    "kubectl get svc -l app=dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the deployment\n",
    "kubectl get deploy -l app=dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the ReplicaSet\n",
    "kubectl get rs -l app=dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show where the Pods have been scheduled\n",
    "kubectl get pods -l app=dashboard -o json | jq '.items[] | {name: .metadata.name, node: .spec.nodeName}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the connection to Dashboard service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the port exposed by Dashboard service\n",
    "export DASHBOARD_PORT=\"$(kubectl get svc -l app=dashboard -o json | jq '.items[0].spec.ports[0].nodePort')\"\n",
    "\n",
    "# perform a \"GET /\" on Dashboard\n",
    "curl http://$MASTER_NODE:$DASHBOARD_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Production\n",
    "## 1. Reaching Core and Dashboard from the outside\n",
    "\n",
    "The exposed services can be accessible from the outside after the SSH client performes a SSH address binding on the remote PaaS VM.\n",
    "\n",
    "**Connect to Core** \n",
    "- Bind the address\n",
    "  ```bash\n",
    "  ssh -NL 8050:$MASTER_NODE:$CORE_PORT $USER@$PAAS_HOST\n",
    "  ```\n",
    "- Connect via web browser at ```http://localhost:8050```\n",
    "\n",
    "**Connect to Dashboard**\n",
    "- Bind the address \n",
    "  ```bash\n",
    "  ssh -NL 8080:$MASTER_NODE:$DASHBOARD_PORT $USER@$PAAS_HOST\n",
    "  ```\n",
    "- Connect via web browser at ```http://localhost:8080```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Master node IP\" $MASTER_NODE\n",
    "echo \"Corestack port\" $CORE_PORT\n",
    "echo \"Dashboard port\" $DASHBOARD_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. New Deployment and Rollout\n",
    "### 2.1. Deployment of v2\n",
    "\n",
    "A new version (v2) for the Corestack is available.\n",
    "\n",
    "Modify the deployment of the Corestack to include the new version of the Docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deploy -l app=core -o yaml | \\\n",
    "sed 's;mcaliandro/smarthome:corev1;mcaliandro/smarthome:corev2;g' | \\\n",
    "kubectl replace -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the details about the new deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deploy -l app=core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the functionalities of Core via web browser at [http://localhost:8050](http://localhost:8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the functionalities of Dashboard via web browser at [http://localhost:8080](http://localhost:8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Rollback Core Deployment to v1\n",
    "\n",
    "Now, Dashboard simply doesn't work with the new version of Core, it would be better to rollback it to the previous version.\n",
    "\n",
    "Get the history of the Core deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl rollout history deploy core-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rollback to v1, i.e., undo the last deployment of v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl rollout undo deployment core-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Balancing\n",
    "\n",
    "The load balancing experiment is done by activating the SHS emulator and the House Owner emulator, that perform a block of HTTP requests every second.\n",
    "\n",
    "Open a new terminal session and monitor the current workload:\n",
    "\n",
    "```bash\n",
    "kubectl get pod -o wide -w\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Activate the SHS Emulator\n",
    "\n",
    "The set of smart home devices can be emulated via a Python script that performs some PUT requests to the Core APIs, to provide the updated information about the system they monitor. \n",
    "\n",
    "Retrieve the connection parameters to Corestack (master node IP address and Core service port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Master node IP\" $MASTER_NODE\n",
    "echo \"Corestack port\" $CORE_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a container (outside the cluster) from a Docker image that includes the mechanism to perform PUT requests every 5 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d \\\n",
    "    -e API_HOST=$MASTER_NODE \\\n",
    "    -e API_PORT=$CORE_PORT \\\n",
    "    --name shsemulator mcaliandro/smarthome:shsemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the container \"shsemulator\" is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker ps -f name=shsemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs within the \"shsemulator\" container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker logs shsemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Activate the House Owner Emulator\n",
    "\n",
    "The workload from the client-side (the traffic to the Dashboard) can be emulated via a Python script that performs some GET requests to the Dashboard, to retrieve the updated information about the smart home system the users own.\n",
    "\n",
    "Create a container (outside the cluster) from a Docker image that includes the mechanism to perform GET requests every second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Master node IP\" $MASTER_NODE\n",
    "echo \"Dashboard port\" $DASHBOARD_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run -d \\\n",
    "    -e WEB_HOST=$MASTER_NODE \\\n",
    "    -e WEB_PORT=$DASHBOARD_PORT \\\n",
    "    --name hoemulator mcaliandro/smarthome:hoemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the container \"hoemulator\" is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker ps -f name=hoemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the logs within the \"hoemulator\" container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker logs hoemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Scale the Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale up the deployment of Core by increasing the number of replicas to 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl scale deploy core-deploy --replicas=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale up the deployment of Dashboard by increasing the number of replicas to 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl scale deploy dashboard-deploy --replicas=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the resources consumed by Core Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe svc -l app=core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the resources consumed by Dashboard Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe svc -l app=dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Downscale the Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the \"shsemulator\" container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker stop shsemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the \"hoemulator\" container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker stop hoemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscale Core replicas back to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deploy core-deploy -o yaml | sed 's/replicas: 12/replicas: 3/g' | kubectl replace -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscale Dashboard replicas back to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deploy dashboard-deploy -o yaml | sed 's/replicas: 6/replicas: 3/g' | kubectl replace -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the \"shsemulator\" container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker rm shsemulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the \"hoemulator\" container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker rm hoemulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind delete cluster --name=$CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unset DB_HOST; unset DB_PORT\n",
    "unset MASTER_NODE; unset CORE_PORT; unset DASHBOARD_PORT\n",
    "unset CLUSTER; unset KUBECONFIG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
